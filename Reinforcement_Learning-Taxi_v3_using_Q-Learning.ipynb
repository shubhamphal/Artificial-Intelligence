{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\").env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print('timestep:',i+1)\n",
    "        print('reward:',frame['reward'])\n",
    "        print('penalty:',frame['penalty'])\n",
    "        print('state:',frame['state'])\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_action_model(state):\n",
    "    frames=[]\n",
    "    penalties=0\n",
    "    done=False\n",
    "    env.s=state\n",
    "    while not done:\n",
    "        action=env.action_space.sample()\n",
    "        state,reward,done,info=env.step(action)\n",
    "        if reward==-10:\n",
    "            penalties+=1\n",
    "        frames.append({'frame':env.render(mode='ansi'),'reward':reward,'penalty':penalties,'state':state})\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_learning_model(state):\n",
    "    frames=[]\n",
    "    penalties=0\n",
    "    done=False\n",
    "    env.s=state\n",
    "    q_table = np.loadtxt('q_table.txt', dtype=float)\n",
    "    while not done:\n",
    "        action=np.argmax(q_table[state])\n",
    "        state,reward,done,info=env.step(action)\n",
    "        if reward==-10:\n",
    "            penalties+=1\n",
    "        frames.append({'frame':env.render(mode='ansi'),'reward':reward,'penalty':penalties,'state':state})\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Random Model v/s Reinforcement Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.encode(3, 1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "env.s = state\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "timestep: 1860\n",
      "reward: 20\n",
      "penalty: 619\n",
      "state: 0\n"
     ]
    }
   ],
   "source": [
    "print_frames(random_action_model(state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "timestep: 10\n",
      "reward: 20\n",
      "penalty: 0\n",
      "state: 0\n"
     ]
    }
   ],
   "source": [
    "print_frames(Q_learning_model(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha=0.1\n",
    "gamma=0.6\n",
    "epsilon=0.1\n",
    "q_table=np.zeros([env.observation_space.n, env.action_space.n])\n",
    "for episode in range (1,100001):\n",
    "    epochs, penalties, reward= 0, 0, 0\n",
    "    state = env.reset()\n",
    "    done=False\n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action=env.action_space.sample()\n",
    "        else:\n",
    "            action=np.argmax(q_table[state])\n",
    "            \n",
    "        next_state, reward, done, info=env.step(action)\n",
    "        \n",
    "        if reward==-10:\n",
    "            penalties+=1\n",
    "        \n",
    "        q_table[state,action]=(1-alpha)*q_table[state,action]+alpha*(reward+gamma*np.max(q_table[next_state]))\n",
    "        \n",
    "        state=next_state\n",
    "        \n",
    "        epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('q_table.txt', q_table, fmt='%f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
